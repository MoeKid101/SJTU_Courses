import jittor as jt
import numpy as np
from jittor.nn import Module

jt.flags.use_cuda = True
CIFAR_ORIG_PATH = 'cifar-10-batches-py'

def genData(fileNames:list, outName:str, doMask:bool=False):
    '''
    The function to generate a file with content (data, label) from the original
    downloaded CIFAR-10 dataset. Here (data, label) are both np.ndarrays with
    shape <data>:(-1, 3, 32, 32) and <label>:(-1).
    '''
    def unpack(filename:str)->dict:
        import pickle
        with open(str.join('/', [CIFAR_ORIG_PATH, filename]), 'rb') as file:
            dict = pickle.load(file, encoding='bytes')
        return dict
    def mask(data:np.ndarray, label:np.ndarray)->tuple[np.ndarray, np.ndarray]:
        label_cond = (label >= 5)
        index_cond = (np.arange(label.shape[0]) % 10 == 0)
        final_cond = np.bitwise_or(label_cond, index_cond)
        return data[final_cond], label[final_cond]
    data_lst, label_lst = list(), list()
    for fileName in fileNames:
        dataDict = unpack(fileName)
        data_lst.append(dataDict[b'data'].reshape((-1, 3, 32, 32)))
        label_lst.append(dataDict[b'labels'])
    dataArr, labelArr = (np.float32(np.concatenate(data_lst, axis=0)) / 256,
                         np.concatenate(label_lst, axis=0))
    if doMask: dataArr, labelArr = mask(dataArr, labelArr)
    jt.save((dataArr, labelArr), outName)

def show(img, path):
    '''
    The function which outputs a certain image. Remember that it accepts only integer
    type arrays.
    '''
    import matplotlib.pyplot as plt
    from PIL import Image
    img = img.reshape((3, 32, 32))
    x = img
    x = np.zeros((32, 32, 3), dtype=int)
    for i in range(3):
        x[:,:,i] = img[i,:,:]
    plt.imshow(x)
    plt.savefig(path)
    plt.clf()

def augment(std_data:np.ndarray, std_label:np.ndarray, numCopy:int=4)->tuple:
    '''
    The function to implement data augmentation. Accepts <std_data>, <std_label>
    with shape (-1, 3, 32, 32) and (-1,) respectively as input.
    '''
    def genEB_2(minlen:int, maxlen:int, num:int):
        from numpy import random
        hSize, vSize = (random.randint(minlen, maxlen+1, size=num),
                        random.randint(minlen, maxlen+1, size=num))
        hMMarg, vMMarg = maxlen - hSize, maxlen - vSize
        hMargCoef, vMargCoef = random.rand(num), random.rand(num)
        hMarg, vMarg = ((hMMarg*hMargCoef).astype(np.int32),
                        (vMMarg*vMargCoef).astype(np.int32))
        ud, lr = random.randint(2, size=num), random.randint(2, size=num)
        ub, db = ud*(31-hSize-hMarg)+(1-ud)*hMarg, ud*(31-hMarg)+(1-ud)*(hMarg+hSize)
        lb, rb = lr*(31-vSize-vMarg)+(1-lr)*vMarg, lr*(31-vMarg)+(1-lr)*(vMarg+vSize)
        return ub, db, lb, rb
    copyShape = (numCopy, 1, 1, 1, 1)
    eraMin, eraMax = 4, 18
    std_data, std_label = std_data.reshape((1, -1, 3, 32, 32)), std_label.reshape((1, -1))
    DataCopy, LabelCopy = np.tile(std_data, copyShape), np.tile(std_label, copyShape)
    for i in range(numCopy):
        if i >= numCopy//2: DataCopy[i] = np.flip(DataCopy[i], axis=3)
        # random erasing
        ub, db, lb, rb = genEB_2(eraMin, eraMax, DataCopy.shape[1])
        for era_index in range(DataCopy.shape[1]):
            DataCopy[i, era_index, :, ub[era_index]:db[era_index],
                     lb[era_index]:rb[era_index]] = 0
    dataRet, labelRet = DataCopy.reshape((-1, 3, 32, 32)), LabelCopy.reshape((-1))
    return dataRet, labelRet

def train_files(model:Module, data_file:str, loss_func, optimizer:jt.optim.Optimizer,
                max_epoch:int, temp_folder:str, from_epoch:int=0, shuffle:bool=True,
                augmentation:bool=True, batch_size:int=128, maxBatchInFile:int=128):
    '''
    This function performs a general training process using a given model and <data_file>
    generated by function <genData>. In every epoch it shuffles and augments the data
    according to parameter <shuffle> and <augmentation>, and then split the whole dataset
    into several temprary files. Afterwards, it train the model on every file generated.
    '''
    import time, os
    from numpy import random
    model.train()
    epoch_losses = list()
    LossSavePath = f'{temp_folder}/loss'
    fin_losses = (jt.load(LossSavePath) if (os.path.exists(LossSavePath) and 
                  from_epoch > 0) else list())
    ModelLoadPath = f'{temp_folder}/model_e{from_epoch}'
    if from_epoch > 0: model.load_state_dict(jt.load(ModelLoadPath))
    dataArr, labelArr = jt.load(data_file)
    for epoch in range(from_epoch, max_epoch+1):
        preProc_start = time.time() # Record start time of pre-processing
        # begin: augment and separate into files
        tmpData, tmpLabel = (augment(dataArr, labelArr) if augmentation
                             else (dataArr, labelArr))
        ### shuffle
        if shuffle:
            shuffle_idx = np.arange(tmpData.shape[0])
            random.shuffle(shuffle_idx)
            tmpData, tmpLabel = tmpData[shuffle_idx], tmpLabel[shuffle_idx]
        ### generate batches and files
        batch_num = tmpData.shape[0] // batch_size
        total_num = batch_num * batch_size
        tmpData, tmpLabel = tmpData[:total_num], tmpLabel[:total_num]
        file_size = maxBatchInFile * batch_size
        file_num = (tmpData.shape[0] // file_size) + 1
        for file_idx in range(file_num):
            fileData, fileLabel = ((tmpData[(file_idx*file_size):((file_idx+1)*file_size)],
                                   tmpLabel[(file_idx*file_size):((file_idx+1)*file_size)])
                                   if file_idx < file_num - 1 else
                                   (tmpData[(file_idx*file_size):], tmpLabel[(file_idx*file_size):]))
            fileDataVAR, fileLabelVAR = (jt.Var(fileData).reshape((-1, batch_size, 3, 32, 32)),
                                         jt.Var(fileLabel).reshape((-1, batch_size)))
            jt.save((fileDataVAR, fileLabelVAR), f'{temp_folder}/data_{file_idx}')
        # end: augment and separate into files
        preProc_end = time.time() # Record end time of pre-processing
        train_start = time.time() # Record start time of training
        # begin: training
        for file_idx in range(file_num):
            dataVAR, labelVAR = jt.load(f'{temp_folder}/data_{file_idx}')
            for batch_idx in range(dataVAR.shape[0]):
                prediction = model(dataVAR[batch_idx])
                loss = loss_func(prediction, labelVAR[batch_idx])
                optimizer.step(loss)
                epoch_losses.append(loss.data[0])
        # end: training
        train_end = time.time() # Record end time of training
        # output necessary information
        epoch_loss_avg = jt.mean(jt.Var(epoch_losses)).data[0]
        epoch_losses.clear()
        fin_losses.append(epoch_loss_avg)
        print(f"Epoch {epoch}. Prepare data {round(preProc_end - preProc_start,2)}s. " + 
              f"Training {round(train_end-train_start, 2)}s with average loss " + 
              f"{np.round(epoch_loss_avg, 5)}.")
        if epoch % 5 == 0:
            jt.save(model.state_dict(), f'{temp_folder}/model_e{epoch}')
            jt.save(fin_losses, LossSavePath)

def plot_conf_mat(actual:np.ndarray, pred:np.ndarray, label_names:list, path:str):
    '''
    The function to plot a confusion matrix according to given labels of prediction and
    ground truth.
    '''
    import matplotlib.pyplot as plt
    from sklearn.metrics import confusion_matrix
    import itertools
    axis_font = {'family':'Consolas', 'color':'darkred', 'size':18}
    cm = confusion_matrix(actual, pred)
    fig, ax = plt.subplots(figsize=(7.5, 7.5))
    ax.matshow(cm, cmap=plt.cm.Blues, alpha=1.0)
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        ax.text(x=j, y=i, s=cm[i,j], va='center', ha='center',
                size='x-large', color=('white' if i==j else 'black'))
    plt.xticks(range(10), label_names)
    plt.yticks(range(10), label_names)
    ax.set_xticklabels(label_names, rotation = 50)
    ax.set_yticklabels(label_names)
    plt.text(3.0, -1.5, 'Prediction', fontdict=axis_font)
    plt.ylabel('Ground Truth', fontdict=axis_font)
    plt.tight_layout()
    plt.savefig(path, dpi=800)
    plt.clf()

def testGenData(model:Module, modelFile:str, dataFile:str, conf_mat_path:str,
                batch_size:int=128):
    '''
    Test the model performance on a <dataFile> constructed by function <genData>.
    Outputs the overall accuracy and plot the confusion matrix using function
    <plot_conf_mat>.
    '''
    def load_names():
        import pickle
        data_dict = {}
        with open(str.join('/', [CIFAR_ORIG_PATH, 'batches.meta']), 'rb') as file:
            data_dict = pickle.load(file, encoding='bytes')
        meta_names = data_dict[b'label_names']
        for i in range(len(meta_names)):
            meta_names[i] = meta_names[i].decode('ascii')
        return meta_names
    dataArr, labelArr = jt.load(dataFile)
    batch_num = dataArr.shape[0] // batch_size
    dataArr, labelArr = dataArr[:batch_num*batch_size], labelArr[:batch_num*batch_size]
    dataVAR, labelVAR = (jt.Var(dataArr).reshape((-1, batch_size, 3, 32, 32)),
                         jt.Var(labelArr).reshape((-1, batch_size)))
    # generate prediction
    model.load_state_dict(jt.load(modelFile))
    model.eval()
    pred_lst, wrong_sum, total_sum = list(), 0, 0
    for batch in range(dataVAR.shape[0]):
        model_output = model(dataVAR[batch])
        prediction = jt.argmax(model_output, dim=1)[0]
        pred_lst.append(prediction)
        # calculate the number of items predicted wrong.
        diff = (prediction != labelVAR[batch]).sum()
        wrong_sum += diff.item()
        total_sum += batch_size
    predVAR = jt.concat(pred_lst, dim=0)
    labelVAR = labelVAR.reshape((-1))
    print(f'{wrong_sum} wrong predictions in {total_sum}.')
    # plot confusion matrix
    labelNames = load_names()
    plot_conf_mat(labelVAR, predVAR, labelNames, conf_mat_path)